{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SureStart Day 21 - UpSampling 2D + Conv2D Transpose Layers Keras Tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAibbofTQL0K"
      },
      "source": [
        "Tutorial credit: https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH4BXzBeQROt"
      },
      "source": [
        "How to use the UpSampling2D Layer: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ejnNVJuSO8z"
      },
      "source": [
        "#importing the necessary packages and configuring some parameters\n",
        "\n",
        "#basic packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "#Packages for data prep\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#Packages for modeling\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "72WAtCQAQUc6",
        "outputId": "6a5acc34-f575-423f-ce42-036082d4b3fe"
      },
      "source": [
        "\"\"\"the simplest way to upsample an input is to double each row and column\n",
        "For example, a 2x2 input image would be output as 4x4\n",
        "\n",
        "         1,2\n",
        "input = (3,4)\n",
        "\n",
        "         1, 1, 2, 2\n",
        "Output: (1, 1, 2, 2)\n",
        "         3, 3, 4, 4\n",
        "         3, 4, 4, 4\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the simplest way to upsample an input is to double each row and column\\nFor example, a 2x2 input image would be output as 4x4\\n\\n         1,2\\ninput = (3,4)\\n\\n         1, 1, 2, 2\\nOutput: (1, 1, 2, 2)\\n         3, 3, 4, 4\\n         3, 4, 4, 4\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2x794txQ1xR"
      },
      "source": [
        "The Keras deep learning library provides this capability in a layer called UpSampling2D.\n",
        "\n",
        "\n",
        "\n",
        "It can be added to a convolutional neural network and repeats the rows and columns provided as input in the output. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-9QasyiQyQn"
      },
      "source": [
        "#define model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import UpSampling2D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(UpSampling2D())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHJG2J-kRAUg"
      },
      "source": [
        "We can demonstrate the behavior of this layer with a simple contrived example.\n",
        "\n",
        "First, we can define a contrived input image that is 2×2 pixels. We can use specific values for each pixel so that after upsampling, we can see exactly what effect the operation had on the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK1EpUvlRAw5",
        "outputId": "6e638dd2-f4af-42ee-f52c-aa3bf0dda25a"
      },
      "source": [
        "#define input data\n",
        "import numpy\n",
        "\n",
        "X = numpy.asarray([[1,2],\n",
        "             [3,4]])\n",
        "#show input data for context\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGaKS0L5Rj4p"
      },
      "source": [
        "Once the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8vR7sYbRSaG"
      },
      "source": [
        "#reshape input data into one sample a sample w/a channel\n",
        "X = X.reshape((1, 2, 2, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy16u0RVRzsf"
      },
      "source": [
        "We can now define our model. The model only has the UpSampling2D layer which takes 2x2 grayscale images as input directly and outputs the result of the upsampling operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzscgIh6R9Ji",
        "outputId": "8eae31ab-44c9-4d5f-ee69-f68240aed590"
      },
      "source": [
        "#make a prediction w/the model\n",
        "yhat = model.predict(X)\n",
        "\n",
        "#reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4,4))\n",
        "\n",
        "#summarize output\n",
        "print(yhat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 2 2]\n",
            " [1 1 2 2]\n",
            " [3 3 4 4]\n",
            " [3 3 4 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk3oaH64TClZ"
      },
      "source": [
        "Running the above cells first creates and summarizes our 2x2 input data.\n",
        "\n",
        "Next, the model is summarized. We can see that it will output a 4x4 result as we expect, and importnatly, the layer has no parameters or model weights. THis is because it is not learning anyting; it is just doubling the input. \n",
        "\n",
        "Finally, the model is used to upsample our input, resulting in a doubling of each row and column for out input data, as we expected. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsX1ZZ9BTZXB",
        "outputId": "1728d6d6-8ddd-43d0-ca24-9958edffdbfd"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "up_sampling2d_1 (UpSampling2 (None, 4, 4, 1)           0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daIrD__VTdqI"
      },
      "source": [
        "By default, the UpSampling2D will double each input dimension. This is defined by the \"size\" argument that is set to the tuple (2,2).\n",
        "\n",
        "\n",
        "\n",
        "You may want to use different factors on each dimension, such as double the width and triple the height. This could be achieved by setting the \"size\" argument to (2,3). The result of applying this operation to a 2x2 image would be a 4x6 output image (e.g. 2x2 and 2x3). For example: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fouMOaQoT2-m",
        "outputId": "621f4759-071d-4941-c978-121e231a65cd"
      },
      "source": [
        "#example of using different scale factors for each dimension\n",
        "model.add(UpSampling2D(size = (2,3)))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "up_sampling2d_1 (UpSampling2 (None, 4, 4, 1)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 8, 12, 1)          0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 16, 36, 1)         0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VmP6_TcUAuF"
      },
      "source": [
        "Additionally, by default, the UpSampling2D layer will use a nearest neighbor algorithm to fill in the new rows and columns. This has teh effect of simply doubling rows and columns, as described and is specified by the \"interpolation\" argument set to \"nearest\". \n",
        "\n",
        "Alternately, a bilinear interpolation method can be used which draws upon multiple surrounding points. This can be specified via setting the \"interpolation\" argument to \"bilinear\". For example: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZXVAmODUekD",
        "outputId": "407354ce-184f-45f5-ade9-0b687bfbc9c2"
      },
      "source": [
        "#example of using bilinear interpolation when upscaling\n",
        "model.add(UpSampling2D(interpolation = 'bilinear'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "up_sampling2d_1 (UpSampling2 (None, 4, 4, 1)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 8, 12, 1)          0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 16, 36, 1)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 32, 72, 1)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2 (None, 64, 144, 1)        0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d40T2nWYUulX"
      },
      "source": [
        "The UpSampling2D layer is simple and effective, although does not perform any learning.\n",
        "\n",
        "It is not able to fill in useful detail in the upsampling operation. To be useful in a GAN, each UpSampling2D layer must be followed by a Conv2D layer that will learn to interpret the doubled input and be trained to translate it into meaningful detail.\n",
        "\n",
        "We can demonstrate this with an example.\n",
        "\n",
        "In this case, our little GAN generator model must produce a 10×10 image and take a 100 element vector from the latent space as input.\n",
        "\n",
        "First, a Dense fully connected layer can be used to interpret the input vector and create a sufficient number of activations (outputs) that can be reshaped into a low-resolution version of our output image, in this case, 128 versions of a 5×5 image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efz6qo2PU0Q9",
        "outputId": "ca779ef2-6308-4fc9-85ef-b76ada16d398"
      },
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import UpSampling2D\n",
        "from keras.layers import Conv2D\n",
        "from keras.models import Sequential\n",
        "\n",
        "#define model\n",
        "model = Sequential()\n",
        "\n",
        "#define input shape, output enough activations for the 128 5x5 image\n",
        "model.add(Dense(128 * 5 * 5, input_dim = 100))\n",
        "\n",
        "#reshape vector of activations into 128 feature maps with 5x5\n",
        "model.add(Reshape((5, 5, 128)))\n",
        "\n",
        "#fill in detail in the unsampled feature maps and output a single image\n",
        "model.add(Conv2D(1, (3,3), padding = 'same'))\n",
        "\n",
        "#summarize model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 3200)              323200    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 5, 5, 1)           1153      \n",
            "=================================================================\n",
            "Total params: 324,353\n",
            "Trainable params: 324,353\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x_IBj2GVwXx"
      },
      "source": [
        "Running the example creates the model and summarizes the output shape of each layer. \n",
        "\n",
        "We can see that the Dense layer outputs 3,200 activations that are then reshaped into 128 feature maps with the shape 5x5. \n",
        "\n",
        "The widths and heights are doubled to 10x10 by the UpSampling 2D layer, resulting in a feature map with quadruple the area. \n",
        "\n",
        "Finally, the Conv2D processes these feature maps and adds in detail, outputting a single 10x10 image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wAoCs4lWL6H"
      },
      "source": [
        "#Conv2D Transpose / Transpose convolution layer\n",
        "The Conv2D Transpose or transpose convolution layer is more complicated than a simple upsampling layer.\n",
        "\n",
        "It performs the unsampling operation AND interprets the raw input data to fill in the detail while it is unsampling. It's like a layer that combines the UpSampling2D and Conv2D layers into one layer. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6u8KubfYIDv"
      },
      "source": [
        "\"\"\"\n",
        "consider an input image with the size 2x2 as follows: \n",
        "\n",
        "        1, 2\n",
        "Input = (3,4)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMtPX4wbX2e3"
      },
      "source": [
        "Assuming a single filter with a 1x1 kernel and modle weights that result in no changes to the inputs when output (e.g. a model weight of 1.0 and a bias of 0.0), a transpose convolution operation with an output stride of 1x1 will reproduce the output as-is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSF1s_yVX-EJ"
      },
      "source": [
        "\"\"\"\n",
        "          1, 2\n",
        "Output = (3, 4)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbe1KFFOYOHx"
      },
      "source": [
        "With an output stride of (2,2), the 1x1 convolution requires the insertion of additional rows and columns into the input image so that the reads of the operation can be performed. Therefore, the input looks as follows: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkOyzdSZYakO"
      },
      "source": [
        "\"\"\"\n",
        "         1, 0, 2, 0\n",
        "Input = (0, 0, 0, 0)\n",
        "         3, 0, 4, 0\n",
        "         0, 0, 0, 0 \n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40CQqs-rYljR"
      },
      "source": [
        "The model can then read across this input using an output stride of (2,2) and will output a 4x4 image, in this case with no change as our model weights have no effect by design:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tCm2OBZYw_l"
      },
      "source": [
        "\"\"\"\n",
        "          1, 0, 2, 0\n",
        "Output = (0, 0, 0, 0)\n",
        "          3, 0, 4, 0\n",
        "          0, 0, 0, 0\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv2XUuZuZARX"
      },
      "source": [
        "Keras provides the transpose convolution capability via the Conv2DTranspose layer. It can be added to our model directly; for example: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyBQ3Z-pZgso"
      },
      "source": [
        "#example of using the transpose convolutional layer\n",
        "from numpy import asarray\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2DTranspose\n",
        "\n",
        "#define model\n",
        "model = Sequential()\n",
        "model.add(Conv2DTranspose(...))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFtgqpq4Zkwe"
      },
      "source": [
        "We can demonstrate the behavior of this layer with a simple contrived example.\n",
        "\n",
        "First, we cna define a contrived input image that is 2x2 pixels, as we did in the previous section. We can use specific values for each pixel so that after the transpose convolutional operation, we can see exactly what effect the operation had on the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59jr2yNVZ09T",
        "outputId": "eb4f4745-8096-49d7-d482-e83359d7eb71"
      },
      "source": [
        "#define input data\n",
        "X = asarray([[1,2],\n",
        "             [3,4]])\n",
        "\n",
        "#show input data for context\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfUqb9MvZ8ol"
      },
      "source": [
        "Once the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11y9Uhm4aFng"
      },
      "source": [
        "#reshape input data into one sample - a sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic3cxHi9aMmM"
      },
      "source": [
        "We can now define our model. \n",
        "\n",
        "The model has only the Conv2DTranspose layer, which takes 2x2 grayscale images as input directly and outputs the result of the operation.\n",
        "\n",
        "The Conv2DTranspose both upsamples and performs a convolution. As such, we must specifiy the both the num of filters and the size of the filters as we do for Conv2D layers. Additionally, we must specify a stride of (2,2) becuase the upsampling is achieved by the stride behaviour of the convolution on the input. \n",
        "\n",
        "Specifying a stride of (2,2) has the effect of spacing out the input. Specifically, rows and columns of 0.0 values are inserted to achieve the desired stride. \n",
        "\n",
        "In this example, we will use one filter, with a 1x1 kernel and a stride of 2x2 so that the 2x2 input image is upsampled to 4x4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXD3AyszbByS",
        "outputId": "26a3bb61-e01e-47e7-b30c-48adf39503f0"
      },
      "source": [
        "#define model\n",
        "model = Sequential()\n",
        "model.add(Conv2DTranspose(1, (1,1), strides = (2,2), input_shape = (2, 2, 1)))\n",
        "\n",
        "#summarize the model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_transpose (Conv2DTran (None, 4, 4, 1)           2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbCn8MNrdLUp"
      },
      "source": [
        "We can demonstrate the behavior of this layer with a simple contrived example.\n",
        "\n",
        "First, we can define a contrived input image that is 2×2 pixels. We can use specific values for each pixel so that after upsampling, we can see exactly what effect the operation had on the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpjBcYHfdUPk",
        "outputId": "39c34055-a684-4245-f569-69a57232610d"
      },
      "source": [
        "# define input data\n",
        "X = asarray([[1, 2],\n",
        "\t\t\t [3, 4]])\n",
        "# show input data for context\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CowWkqM6da1k"
      },
      "source": [
        "Once the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDaWmnXTdbn_"
      },
      "source": [
        "# reshape input data into one sample a sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HGLRL0LdpMi"
      },
      "source": [
        "We can now define our model.\n",
        "\n",
        "The model has only the Conv2DTranspose layer, which takes 2×2 grayscale images as input directly and outputs the result of the operation.\n",
        "\n",
        "The Conv2DTranspose both upsamples and performs a convolution. As such, we must specify both the number of filters and the size of the filters as we do for Conv2D layers. Additionally, we must specify a stride of (2,2) because the upsampling is achieved by the stride behavior of the convolution on the input.\n",
        "\n",
        "Specifying a stride of (2,2) has the effect of spacing out the input. Specifically, rows and columns of 0.0 values are inserted to achieve the desired stride.\n",
        "\n",
        "In this example, we will use one filter, with a 1×1 kernel and a stride of 2×2 so that the 2×2 input image is upsampled to 4×4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhD-7PFtdqDl",
        "outputId": "b1552bc7-b157-4854-cdbe-f3770c2ae321"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Conv2DTranspose(1, (1,1), strides=(2,2), input_shape=(2, 2, 1)))\n",
        "# summarize the model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 1)           2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnzCE57hduuY"
      },
      "source": [
        "To make it clear what the Conv2DTranspose layer is doing, we will fix the single weight in the single filter to the value of 1.0 and use a bias value of 0.0.\n",
        "\n",
        "These weights, along with a kernel size of (1,1) will mean that values in the input will be multiplied by 1 and output as-is, and the 0 values in the new rows and columns added via the stride of 2×2 will be output as 0 (e.g. 1 * 0 in each case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut1WtnlJd3Ib"
      },
      "source": [
        "# define weights that they do nothing\n",
        "weights = [asarray([[[[1]]]]), asarray([0])]\n",
        "# store the weights in the model\n",
        "model.set_weights(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc0AWPvPd5Ce"
      },
      "source": [
        "We can then use the model to make a prediction, that is upsample a provided input image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4vVwG6wfccw"
      },
      "source": [
        "# make a prediction with the model\n",
        "yhat = model.predict(X)\n",
        "\n",
        "# reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 4))\n",
        "\n",
        "# summarize output\n",
        "print(yhat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz8NnHzvfyCM"
      },
      "source": [
        "The full example is typed below: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0WaqDOift7L",
        "outputId": "1c7145af-e769-4508-a8f4-71c8a0821759"
      },
      "source": [
        "# example of using the transpose convolutional layer\n",
        "from numpy import asarray\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2DTranspose\n",
        "# define input data\n",
        "X = asarray([[1, 2],\n",
        "\t\t\t [3, 4]])\n",
        "# show input data for context\n",
        "print(X)\n",
        "# reshape input data into one sample a sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Conv2DTranspose(1, (1,1), strides=(2,2), input_shape=(2, 2, 1)))\n",
        "# summarize the model\n",
        "model.summary()\n",
        "# define weights that they do nothing\n",
        "weights = [asarray([[[[1]]]]), asarray([0])]\n",
        "# store the weights in the model\n",
        "model.set_weights(weights)\n",
        "# make a prediction with the model\n",
        "yhat = model.predict(X)\n",
        "# reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 4))\n",
        "# summarize output\n",
        "print(yhat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_transpose_2 (Conv2DTr (None, 4, 4, 1)           2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[1. 0. 2. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [3. 0. 4. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsil5yBhf1I6"
      },
      "source": [
        "Running the example first creates and summarizes our 2×2 input data.\n",
        "\n",
        "Next, the model is summarized. We can see that it will output a 4×4 result as we expect, and importantly, the layer two parameters or model weights. One for the single 1×1 filter and one for the bias. Unlike the UpSampling2D layer, the Conv2DTranspose will learn during training and will attempt to fill in detail as part of the upsampling process.\n",
        "\n",
        "Finally, the model is used to upsample our input. We can see that the calculations of the cells that involve real values as input result in the real value as output (e.g. 1×1, 1×2, etc.). We can see that where new rows and columns have been inserted by the stride of 2×2, that their 0.0 values multiplied by the 1.0 values in the single 1×1 filter have resulted in 0 values in the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQkIHPbLgDcC"
      },
      "source": [
        "Remember: this is a contrived case where we artificially specified the model weights so that we could see the effect of the transpose convolutional operation.\n",
        "\n",
        "In practice, we will use a large number of filters (e.g. 64 or 128), a larger kernel (e.g. 3×3, 5×5, etc.), and the layer will be initialized with random weights that will learn how to effectively upsample with detail during training.\n",
        "\n",
        "In fact, you might imagine how different sized kernels will result in different sized outputs, more than doubling the width and height of the input. In this case, the ‘padding‘ argument of the layer can be set to ‘same‘ to force the output to have the desired (doubled) output shape; for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_9Iph0dgENy",
        "outputId": "651bf9dc-0cc3-49aa-bb6d-a6bad6d0f92d"
      },
      "source": [
        "# example of using padding to ensure that the output is only doubled\n",
        "model.add(Conv2DTranspose(1, (3,3), strides=(2,2), padding='same', input_shape=(2, 2, 1)))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_transpose_2 (Conv2DTr (None, 4, 4, 1)           2         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 8, 8, 1)           10        \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTr (None, 16, 16, 1)         10        \n",
            "=================================================================\n",
            "Total params: 22\n",
            "Trainable params: 22\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js9jXMeagKQp"
      },
      "source": [
        "#Simple Generator Model With the Conv2DTranspose Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx9kC3gPgrYX"
      },
      "source": [
        "The Conv2DTranspose is more complex than the UpSampling2D layer, but it is also effective when used in GAN models, specifically the generator model.\n",
        "\n",
        "Either approach can be used, although the Conv2DTranspose layer is preferred, perhaps because of the simpler generator models and possibly better results, although GAN performance and skill is notoriously difficult to quantify.\n",
        "\n",
        "We can demonstrate using the Conv2DTranspose layer in a generator model with another simple example.\n",
        "\n",
        "In this case, our little GAN generator model must produce a 10×10 image and take a 100-element vector from the latent space as input, as in the previous UpSampling2D example.\n",
        "\n",
        "First, a Dense fully connected layer can be used to interpret the input vector and create a sufficient number of activations (outputs) that can be reshaped into a low-resolution version of our output image, in this case, 128 versions of a 5×5 image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYvVpX3egsQn"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "# define input shape, output enough activations for for 128 5x5 image\n",
        "model.add(Dense(128 * 5 * 5, input_dim=100))\n",
        "# reshape vector of activations into 128 feature maps with 5x5\n",
        "model.add(Reshape((5, 5, 128)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv75D4pDguMe"
      },
      "source": [
        "Next, the 5×5 feature maps can be upsampled to a 10×10 feature map.\n",
        "\n",
        "We will use a 3×3 kernel size for the single filter, which will result in a slightly larger than doubled width and height in the output feature map (11×11).\n",
        "\n",
        "Therefore, we will set ‘padding‘ to ‘same’ to ensure the output dimensions are 10×10 as required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV7GCDDFgzzO"
      },
      "source": [
        "# double input from 128 5x5 to 1 10x10 feature map\n",
        "model.add(Conv2DTranspose(1, (3,3), strides=(2,2), padding='same'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdPLYQ14g2X5"
      },
      "source": [
        "Tying this together, the complete example is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0cDTlA7g6yW",
        "outputId": "3b76bd57-ec3c-4da2-af85-65ab7ae631f4"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import Conv2D\n",
        "# define model\n",
        "model = Sequential()\n",
        "# define input shape, output enough activations for for 128 5x5 image\n",
        "model.add(Dense(128 * 5 * 5, input_dim=100))\n",
        "# reshape vector of activations into 128 feature maps with 5x5\n",
        "model.add(Reshape((5, 5, 128)))\n",
        "# double input from 128 5x5 to 1 10x10 feature map\n",
        "model.add(Conv2DTranspose(1, (3,3), strides=(2,2), padding='same'))\n",
        "# summarize model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3200)              323200    \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTr (None, 10, 10, 1)         1153      \n",
            "=================================================================\n",
            "Total params: 324,353\n",
            "Trainable params: 324,353\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDK9Uq5Rg_4H"
      },
      "source": [
        "Running the example creates the model and summarizes the output shape of each layer.\n",
        "\n",
        "We can see that the Dense layer outputs 3,200 activations that are then reshaped into 128 feature maps with the shape 5×5.\n",
        "\n",
        "The widths and heights are doubled to 10×10 by the Conv2DTranspose layer resulting in a single feature map with quadruple the area."
      ]
    }
  ]
}