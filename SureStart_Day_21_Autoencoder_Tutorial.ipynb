{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SureStart Day 21 - Autoencoder Tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmz9OZ0JhxLu"
      },
      "source": [
        "Tutorial Credit: https://blog.keras.io/building-autoencoders-in-keras.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJmThYSCufWI"
      },
      "source": [
        "Let's build the simplest possible autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nEqXpyMux0x"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "#This is the size of our encoded representations\n",
        "encoding_dim = 32 #32 floats -> compression of factor 24.5, assuming the input is 784 floats \n",
        "\n",
        "#This is our input image\n",
        "input_img = keras.Input(shape = (784,))\n",
        "\n",
        "#\"encoded\" is the encoded representation of the input\n",
        "encoded = layers.Dense(encoding_dim, activation = 'relu')(input_img)\n",
        "\n",
        "#\"decoded\" is the lossy reconstruction of the input\n",
        "decoded = layers.Dense(784, activation = 'sigmoid')(encoded)\n",
        "\n",
        "#This model maps an input to its reconstruction\n",
        "autoencoder = keras.Model(input_img, decoded)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BZzYz-awC1r"
      },
      "source": [
        "Let's also create a separate encoder model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krbm0vSVwJp7"
      },
      "source": [
        "#This model maps an input to its encoded representation\n",
        "encoder = keras.Model(input_img, encoded)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4AnaFr1wTXl"
      },
      "source": [
        "Let's also create a separate decoder model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDFkBiwOwaCe"
      },
      "source": [
        "#This is our encoded (32-dimensional) input\n",
        "encoded_input = keras.Input(shape = (encoding_dim,))\n",
        "\n",
        "#Retrieve the las layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "\n",
        "#Create the decoder model\n",
        "decoder = keras.Model(encoded_input, decoder_layer,(encoded_input))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfSNI_2cwx3h"
      },
      "source": [
        "Now let's train our autoencoder to reconstruct MNIST digits. \n",
        "\n",
        "First, we'll configure our model to use a per-pixel binary crossentropy loss, and the Adam optimizer: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7OVDukgxImk"
      },
      "source": [
        "autoencoder.compile(optimizer = 'adam', loss = 'binary_crossentropy')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBPfsQ4QymI-"
      },
      "source": [
        "Let's prepare our input data. We're using MNIST digits, and we're discarding the labels (since we're only interested in encoding/decoding the input images)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwfBPpPEywUI",
        "outputId": "15d9f30b-01b0-4339-aa6b-030aff42691b"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(xtrain, _), (xtest, _) = mnist.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4d1U-W4y6km"
      },
      "source": [
        "We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXCI_A1WzGUI",
        "outputId": "0b8e8cda-e3b5-4bfb-f147-49a1c648cdf5"
      },
      "source": [
        "xtrain = xtrain.astype('float32') / 255.\n",
        "xtest = xtest.astype('float32') / 255.\n",
        "xtrain = xtrain.reshape((len(xtrain), np.prod(xtrain.shape[1:])))\n",
        "xtest = xtest.reshape((len(xtest), np.prod(xtest.shape[1:])))\n",
        "print(xtrain.shape)\n",
        "print(xtest.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqt0Z9P2z3pj"
      },
      "source": [
        "Now let's train our autoencoder for 50 epochs: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnQs0xCmz7jD",
        "outputId": "bc6f3a57-da05-4e22-f10b-70359ca7ef47"
      },
      "source": [
        "autoencoder.fit(xtrain, xtrain,\n",
        "               epochs = 50,\n",
        "               batch_size = 256,\n",
        "               shuffle = True,\n",
        "               validation_data = (xtest, xtest))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "235/235 [==============================] - 15s 6ms/step - loss: 0.3862 - val_loss: 0.1882\n",
            "Epoch 2/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1781 - val_loss: 0.1527\n",
            "Epoch 3/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1485 - val_loss: 0.1332\n",
            "Epoch 4/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1315 - val_loss: 0.1209\n",
            "Epoch 5/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1199 - val_loss: 0.1124\n",
            "Epoch 6/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1124 - val_loss: 0.1066\n",
            "Epoch 7/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1071 - val_loss: 0.1026\n",
            "Epoch 8/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1031 - val_loss: 0.0995\n",
            "Epoch 9/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.1002 - val_loss: 0.0971\n",
            "Epoch 10/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0981 - val_loss: 0.0954\n",
            "Epoch 11/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0965 - val_loss: 0.0943\n",
            "Epoch 12/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0955 - val_loss: 0.0936\n",
            "Epoch 13/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0944 - val_loss: 0.0932\n",
            "Epoch 14/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0943 - val_loss: 0.0928\n",
            "Epoch 15/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0940 - val_loss: 0.0926\n",
            "Epoch 16/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0938 - val_loss: 0.0925\n",
            "Epoch 17/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0936 - val_loss: 0.0924\n",
            "Epoch 18/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0932 - val_loss: 0.0922\n",
            "Epoch 19/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0935 - val_loss: 0.0922\n",
            "Epoch 20/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0934 - val_loss: 0.0920\n",
            "Epoch 21/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0931 - val_loss: 0.0920\n",
            "Epoch 22/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0932 - val_loss: 0.0920\n",
            "Epoch 23/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0934 - val_loss: 0.0919\n",
            "Epoch 24/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0931 - val_loss: 0.0918\n",
            "Epoch 25/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0930 - val_loss: 0.0919\n",
            "Epoch 26/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0918\n",
            "Epoch 27/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0918\n",
            "Epoch 28/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0930 - val_loss: 0.0917\n",
            "Epoch 29/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0931 - val_loss: 0.0917\n",
            "Epoch 30/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 31/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 32/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 33/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 34/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.0917\n",
            "Epoch 35/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0927 - val_loss: 0.0917\n",
            "Epoch 36/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 37/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0925 - val_loss: 0.0916\n",
            "Epoch 38/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 39/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 40/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 41/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 42/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 43/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 44/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 45/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0925 - val_loss: 0.0915\n",
            "Epoch 46/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 47/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0925 - val_loss: 0.0914\n",
            "Epoch 48/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0926 - val_loss: 0.0914\n",
            "Epoch 49/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 50/50\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.0926 - val_loss: 0.0915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efc720edb10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHryn83f0RGG"
      },
      "source": [
        "After 50 epochs, the autoencoder seems to reach a stable train/validation loss value of about 0.09. We can try to visualize the reconstructioned inputs and the enconded representations. We will use Matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "z4_rI5tK0exU",
        "outputId": "2556a11e-6bdd-4ebf-c7f5-d678b0549aea"
      },
      "source": [
        "#Encode and decode some digits\n",
        "#Note that we take them from the *test* set\n",
        "encoded_imgs = encoder.predict(xtest)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8fcbb9ae62e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Note that we take them from the *test* set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoded_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdecoded_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1700\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m-> 3022\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3439\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3440\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3441\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3363\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3364\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1544 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1527 run_step  *\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1500 predict_step  *\n        return self(x, training=False)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:1006 __call__  *\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:451 call  *\n        raise NotImplementedError('When subclassing the `Model` class, you should '\n\n    NotImplementedError: When subclassing the `Model` class, you should implement a `call` method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "E4N6unYy25Vm",
        "outputId": "b67c9986-0b1b-4693-935e-74417e92bbd7"
      },
      "source": [
        "# Use Matplotlib (don't ask)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # How many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(xtest[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-c3d5ae9feadf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Display reconstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'decoded_imgs' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}